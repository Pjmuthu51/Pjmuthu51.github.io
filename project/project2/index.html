<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Pragat Jay Muthu" />
    
    <link rel="shortcut icon" type="image/x-icon" href="../../img/favicon.ico">
    <title>Project 2: Modeling, Testing, and Predicting</title>
    <meta name="generator" content="Hugo 0.83.1" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="../../css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">
      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="../../post/">BLOG</a></li>
        
        <li><a href="../../projects/">PROJECTS</a></li>
        
        <li><a href="../../resume/">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="../../project/project2/">Project 2: Modeling, Testing, and Predicting</a></strong>
          </h3>
        </div>
 
<div class="blog-title">
          <h4>
         January 1, 0001 
            &nbsp;&nbsp;
            
          </h4>
        </div>

        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              


<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>The dataset I used comes from the package AES and is originating from the US National Medical Expenditure Survey done in 1987 and 1989. This survey targeted 4,406 individuals who are older than 65 years old (indicating that they are covered by Medicare) that were admitted to long term care facilities. It tracks 19 variables including the number of various types of visits per patient (which I have combined into one overall Total Healthcare Visits variable), alongside data such as their self-reported health, number of chronic conditions, disability, physical region, age, gender, education level, marriage, income, job status, insurance and medicaid status. Overall, I would like to analyze the connections between these various personal characteristics with the amount of hospital visits that these patients take.</p>
</div>
<div id="manova" class="section level3">
<h3>MANOVA</h3>
<pre class="r"><code>Visits &lt;- NMES1988.edited  %&gt;% mutate(age = age *10) %&gt;% select(8:21)

#Manova
MANOVA_dat &lt;- manova(cbind(total.healthcare.visits, chronic, age, school, income)~health, data= Visits)
summary(MANOVA_dat)</code></pre>
<pre><code>## Df Pillai approx F num Df den Df Pr(&gt;F)
## health 2 0.16425 78.737 10 8800 &lt; 2.2e-16 ***
## Residuals 4403
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>#univariate Anova
summary.aov(MANOVA_dat)</code></pre>
<pre><code>## Response total.healthcare.visits :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## health 2 15708 7853.9 54.049 &lt; 2.2e-16 ***
## Residuals 4403 639803 145.3
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response chronic :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## health 2 1049.3 524.64 331.21 &lt; 2.2e-16 ***
## Residuals 4403 6974.5 1.58
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response age :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## health 2 1959 979.46 24.675 2.204e-11 ***
## Residuals 4403 174771 39.69
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response school :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## health 2 1694 846.8 62.265 &lt; 2.2e-16 ***
## Residuals 4403 59880 13.6
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response income :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## health 2 410 204.781 24.193 3.551e-11 ***
## Residuals 4403 37269 8.464
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<p><em>Since the MANOVA test shows that there is significance, univariate ANOVA’s were done. These show that each variable tested (number of total healthcare visits, number of chronic conditions, age, years of education, and income) all show mean differences across various levels of health. When discussing the assumptions for the manova test, the data we have is random and independent, the data is linear between some variables, the data shouldn’t have extreme outliers. When considering normality and equal variance, this data does not show that. This could mean that any conclusions gained from the MANOVA may be flawed.</em></p>
<pre class="r"><code>#post-hoc
pairwise.t.test(Visits$total.healthcare.visits,Visits$health, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  Visits$total.healthcare.visits and Visits$health 
## 
##           average excellent
## excellent 3.6e-06 -        
## poor      &lt; 2e-16 &lt; 2e-16  
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(Visits$chronic,Visits$health, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  Visits$chronic and Visits$health 
## 
##           average excellent
## excellent &lt;2e-16  -        
## poor      &lt;2e-16  &lt;2e-16   
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(Visits$age,Visits$health, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  Visits$age and Visits$health 
## 
##           average excellent
## excellent 0.036   -        
## poor      1.4e-10 2.0e-09  
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(Visits$school,Visits$health, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  Visits$school and Visits$health 
## 
##           average excellent
## excellent 1.5e-05 -        
## poor      &lt; 2e-16 &lt; 2e-16  
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(Visits$income,Visits$health, p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  Visits$income and Visits$health 
## 
##           average excellent
## excellent 1.1e-05 -        
## poor      1.2e-06 7.6e-12  
## 
## P value adjustment method: none</code></pre>
<p><em>When first doing post-hoc analysis, it shows that there are differences between all groups in all variables when using the original alpha of .05.</em></p>
<pre class="r"><code>#prob of type I error
1-(.95)^24</code></pre>
<pre><code>## [1] 0.708011</code></pre>
<pre class="r"><code>#bon
.05/ 24</code></pre>
<pre><code>## [1] 0.002083333</code></pre>
<p><em>However, we did 1 MANOVA, 5 ANOVA’s, and 3x5 post-hoc analyses. This means that the probability of one type I error is 0.708011. To reduce this, we will adjust the significance level using a bonferoni correction with a new alpha of 0.002083333.</em></p>
<pre class="r"><code>#post-hoc- corrected
pairwise.t.test(Visits$total.healthcare.visits,Visits$health, p.adj=&quot;none&quot;)$p.value&lt;0.05/24</code></pre>
<pre><code>##           average excellent
## excellent    TRUE        NA
## poor         TRUE      TRUE</code></pre>
<pre class="r"><code>pairwise.t.test(Visits$chronic,Visits$health, p.adj=&quot;none&quot;)$p.value&lt;0.05/24</code></pre>
<pre><code>##           average excellent
## excellent    TRUE        NA
## poor         TRUE      TRUE</code></pre>
<pre class="r"><code>pairwise.t.test(Visits$age,Visits$health, p.adj=&quot;none&quot;)$p.value&lt;0.05/24</code></pre>
<pre><code>##           average excellent
## excellent   FALSE        NA
## poor         TRUE      TRUE</code></pre>
<pre class="r"><code>pairwise.t.test(Visits$school,Visits$health, p.adj=&quot;none&quot;)$p.value&lt;0.05/24</code></pre>
<pre><code>##           average excellent
## excellent    TRUE        NA
## poor         TRUE      TRUE</code></pre>
<pre class="r"><code>pairwise.t.test(Visits$income,Visits$health, p.adj=&quot;none&quot;)$p.value&lt;0.05/24</code></pre>
<pre><code>##           average excellent
## excellent    TRUE        NA
## poor         TRUE      TRUE</code></pre>
<pre class="r"><code># Perform a MANOVA testing whether any of your numeric variables (or a subset of them, if including them all is unreasonable or doesn&#39;t make sense) show a mean difference across levels of one of your categorical variables (3). If they do, perform univariate ANOVAs to find response(s) showing a mean difference across groups (3), and perform post-hoc t tests to find which groups differ (3). Discuss the number of tests you have performed, calculate the probability of at least one type I error (if unadjusted), and adjust the significance level accordingly (bonferroni correction) before discussing significant differences (3). Briefly discuss some of the MANOVA assumptions and whether or not they are likely to have been met here (no need for anything too in-depth) (2).</code></pre>
<p><em>When using the new p-value, we see that the comparison between average health and excellent health across income levels is no longer significant, while no other significant change.</em></p>
</div>
<div id="randomization-test" class="section level3">
<h3>Randomization Test</h3>
<pre class="r"><code>Visits %&gt;% group_by(insurance) %&gt;% count()</code></pre>
<pre><code>## # A tibble: 2 x 2
## # Groups:   insurance [2]
##   insurance     n
##   &lt;fct&gt;     &lt;int&gt;
## 1 no          985
## 2 yes        3421</code></pre>
<pre class="r"><code>set.seed(348)
library(vegan)
dists &lt;- Visits%&gt;%select(total.healthcare.visits, income)%&gt;%dist
adonis(dists~insurance,data=Visits)</code></pre>
<pre><code>##
## Call:
## adonis(formula = dists ~ insurance, data = Visits)
##
## Permutation: free
## Number of permutations: 999
##
## Terms added sequentially (first to last)
##
## Df SumsOfSqs MeanSqs F.Model R2 Pr(&gt;F)
## insurance 1 2952 2951.93 18.834 0.00426 0.001 ***
## Residuals 4404 690237 156.73 0.99574
## Total 4405 693189 1.00000
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>SST&lt;- sum(dists^2)/4406
SSW&lt;-Visits%&gt;%group_by(insurance)%&gt;% select(insurance,total.healthcare.visits,income)%&gt;%
  do(d=dist(.[-1],&quot;euclidean&quot;))%&gt;%ungroup()%&gt;%
  summarize(sum(d[[1]]^2)/985 + sum(d[[2]]^2)/3421)%&gt;%pull

F_obs&lt;-((SST-SSW)/(2-1))/(SSW/(4406-2))

Fstat&lt;-replicate(100,{
  new &lt;- Visits%&gt;%mutate(insurance=sample(insurance))
  SSW &lt;- new%&gt;%group_by(insurance)%&gt;%select(insurance,total.healthcare.visits, income)%&gt;%
    do(d=dist(.[-1],&quot;euclidean&quot;))%&gt;%ungroup()%&gt;%
    summarize(sum(d[[1]]^2)/985 + sum(d[[2]]^2)/3421)%&gt;%pull
  ((SST-SSW)/1)/(SSW/4404)
})

{hist(Fstat,prob = T); abline(v=F_obs, col=&quot;blue&quot;, add=T)}</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-6-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mean(Fstat&gt;F_obs)</code></pre>
<pre><code>## [1] 0</code></pre>
<p><em>The null hypothesis is that mean total healthcare visits and mean income are equal across different levels of insurance(I.E yes insurance and no insurance). The alternative hypothesis is that mean total healthcare visits and mean income are not equal across different levels of insurance. To test this, we did a MANOVA in a randomization test. When looking at the results, it seems that the total healthcare visits and income are not equal across different levels of insurance, with a p value of .001.</em></p>
</div>
<div id="linear-regression" class="section level3">
<h3>Linear Regression</h3>
<pre class="r"><code>visits_lin &lt;- Visits %&gt;% mutate(cent_vis = total.healthcare.visits-mean(total.healthcare.visits), income_c= income- mean(income))

fit &lt;- lm(cent_vis ~ income_c*adl, data = visits_lin)

summary(fit)</code></pre>
<pre><code>##
## Call:
## lm(formula = cent_vis ~ income_c * adl, data =
visits_lin)
##
## Residuals:
## Min 1Q Median 3Q Max
## -11.960 -6.551 -2.848 2.482 287.414
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 2.50218 0.41650 6.008 2.04e-09 ***
## income_c -0.08814 0.17795 -0.495 0.620
## adlnormal -3.16195 0.46427 -6.811 1.10e-11 ***
## income_c:adlnormal 0.12924 0.19019 0.680 0.497
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 12.13 on 4402 degrees of
freedom
## Multiple R-squared: 0.01135, Adjusted R-squared: 0.01068
## F-statistic: 16.85 on 3 and 4402 DF, p-value: 7.022e-11</code></pre>
<pre class="r"><code>ggplot(visits_lin, aes(x= income_c, y=cent_vis, group= adl)) + geom_smooth(method = &quot;lm&quot;, aes(color=adl))</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-7-1.png" width="768" style="display: block; margin: auto;" />
<em>We fit a linear regression model between the total healthcare visits and income across different ADLs. When looking at the model, we can see the individual predictors and their coefficient values. Intercept indicates that when one has an average income and is limited in activities of daily living, they will have about 2.5 more healthcare visits than the average. Further, when looking at income, for every 10 thousand dollars increase in family income, they will have 0.08814 less healthcare visits. This indicates that increases in income decreases healthcare visits. Similarly, when looking at individuals who are not limited in activites of daily living, they have 3.16195 less healthcare visits than individuals at average income with limitations. This makes sense as limitations usually indicate that someone has health issues that may require more visits. When looking at the interaction between ADLnormal and income, with every 10k increase in family income or a change in limitation status from limited to normal, there is a .129 increase in hospital visits. This was interesting as above we see that these kind of changes decrease number of hospital visits. Something about the interaction between the two changes this, which could be due to the lack of significance of income. This model does only explain about 1.07% of the variation in the total healthcare visits which may be improved with further addition of other variables or variables with more levels.</em></p>
<pre class="r"><code>#assumptions

#linearity
ggplot(Visits) + geom_point(aes(x= income, y=total.healthcare.visits))</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-8-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Residual normality
residuals &lt;- fit$residuals
shapiro.test(residuals)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals
## W = 0.61273, p-value &lt; 2.2e-16</code></pre>
<pre class="r"><code>#homoskedasticity
bptest(fit)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  fit
## BP = 0.97241, df = 3, p-value = 0.8079</code></pre>
<p><em>When looking at the assumptions, we will focus on linearity, residual normality and homoskedasticity. A scatterplot of the data does not show linearity. This could potentially be improved with a logarithmic transformation of the data. For residual normality, we did a shapiro-wilkes test that showed significance indicating that it is not normal. Finally, for homoskedasticity, we ran a BP test that indicated that the assumption of homoskedasticity was met.</em></p>
<pre class="r"><code>summary(fit)$coef</code></pre>
<pre><code>## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 2.5021831 0.4165046 6.0075766 2.035142e-09
## income_c -0.0881409 0.1779544 -0.4953005 6.204128e-01
## adlnormal -3.1619535 0.4642695 -6.8105985 1.103033e-11
## income_c:adlnormal 0.1292412 0.1901902 0.6795365
4.968337e-01</code></pre>
<pre class="r"><code>coeftest(fit, vcov = vcovHC(fit))</code></pre>
<pre><code>##
## t test of coefficients:
##
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 2.502183 0.446283 5.6067 2.188e-08 ***
## income_c -0.088141 0.154215 -0.5715 0.5677
## adlnormal -3.161953 0.488634 -6.4710 1.080e-10 ***
## income_c:adlnormal 0.129241 0.162409 0.7958 0.4262
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<p><em>When re-running the regression using robust standard errors, we see very little change in the coefficients. However, we do see some changes in the standard errors. The intercept SE increases, corresponding to an increase in P-value. The income_c SE decreases, corresponding to an decrease in P-value. The adlnormal SE increases, corresponding to an increase in P-value. Finally, the SE for the interaction between income_c and ADLnormal decreases which corresponds to a decrease in p-value.</em></p>
</div>
<div id="bootstrapped-linear-regression" class="section level3">
<h3>Bootstrapped Linear Regression</h3>
<pre class="r"><code>set.seed(348)
  fit2&lt;-lm(cent_vis ~ income_c*adl, data = visits_lin)
  resids&lt;-fit2$residuals
  fitted&lt;-fit2$fitted.values 
  
  resid_resamp&lt;-replicate(5000,{
    new_resids&lt;-sample(resids,replace=TRUE)
    visits_lin$new_cent_vis&lt;-fitted+new_resids 
    fit2&lt;-lm(new_cent_vis~income_c*adl, data=visits_lin)
    coef(fit2)
})
  
resid_resamp%&gt;%t%&gt;%as.data.frame%&gt;%summarize_all(sd)%&gt;%pivot_longer(1:4,names_to = &quot;Coefficient&quot;, values_to = &quot;Bootstrapped SE&quot;)</code></pre>
<pre><code>## # A tibble: 4 x 2
##   Coefficient        `Bootstrapped SE`
##   &lt;chr&gt;                          &lt;dbl&gt;
## 1 (Intercept)                    0.414
## 2 income_c                       0.180
## 3 adlnormal                      0.460
## 4 income_c:adlnormal             0.191</code></pre>
<pre class="r"><code>#non-bootstrap robust SE
coeftest(fit, vcov = vcovHC(fit))</code></pre>
<pre><code>##
## t test of coefficients:
##
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 2.502183 0.446283 5.6067 2.188e-08 ***
## income_c -0.088141 0.154215 -0.5715 0.5677
## adlnormal -3.161953 0.488634 -6.4710 1.080e-10 ***
## income_c:adlnormal 0.129241 0.162409 0.7958 0.4262
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>#non-bootstrap SE
summary(fit)$coef</code></pre>
<pre><code>## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 2.5021831 0.4165046 6.0075766 2.035142e-09
## income_c -0.0881409 0.1779544 -0.4953005 6.204128e-01
## adlnormal -3.1619535 0.4642695 -6.8105985 1.103033e-11
## income_c:adlnormal 0.1292412 0.1901902 0.6795365
4.968337e-01</code></pre>
<p><em>When re-running the linear model with bootstrapped SE’s, there are slight differences. The bootstrapped SE’s are closer to the non-robust SE’s. Specifically, the intercept for the bootstrapped SE is .414, which is closer to the npn-robust SE of .417 vs the robust model’s .446. This trend continues with each of the coefficients, indicating that the bootstrapped values potentially have p values closer to the non-robust model’s coefficient’s p values.</em></p>
</div>
<div id="logistic-model" class="section level3">
<h3>Logistic model</h3>
<pre class="r"><code>data &lt;- Visits %&gt;% mutate(y = recode(medicaid, &quot;yes&quot; = 1, &quot;no&quot; = 0)) %&gt;% mutate(school_c = school-mean(school), income_c= income- mean(income))

limlog&lt;-glm(y~income_c+school_c, data=data, family=binomial)
summary(limlog)</code></pre>
<pre><code>##
## Call:
## glm(formula = y ~ income_c + school_c, family =
binomial, data = data)
##
## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.4181 -0.4584 -0.2968 -0.1421 4.9175
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -3.16189 0.10408 -30.378 &lt;2e-16 ***
## income_c -0.60754 0.06243 -9.731 &lt;2e-16 ***
## school_c -0.21151 0.01522 -13.897 &lt;2e-16 ***
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 2691.1 on 4405 degrees of freedom
## Residual deviance: 2223.1 on 4403 degrees of freedom
## AIC: 2229.1
##
## Number of Fisher Scoring iterations: 7</code></pre>
<pre class="r"><code>exp(coeftest(limlog))</code></pre>
<pre><code>## 
## z test of coefficients:
## 
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.042346   1.109693   0e+00        1
## income_c    0.544689   1.064425   1e-04        1
## school_c    0.809358   1.015336   0e+00        1</code></pre>
<p><em>When creating a logistic model for if someone has medicaid based off of income and number of years of education, we get an intercept reference group of individuals with average education and average family income. To interpret the coefficients, we exponetiated each one. When looking at income, we see that for every 10k above average income one has, there is about a 45.5% decrease in odds of having medicaid. This makes sense as medicaid is a healthcare insurance program for low income individuals. Similarly, for every year of education above average, there is about a 19.17% decrease in odds of having medicaid. This also makes sense as income usually increases with increased education, but this is not a guarantee.</em></p>
<pre class="r"><code>#confusion matrix

data$prob &lt;- predict(limlog,type=&quot;response&quot;)
data$predicted &lt;- ifelse(data$prob&gt;.5,&quot;No Medicaid&quot;,&quot;Yes Medicaid&quot;)
table(truth=data$y, prediction=data$predicted)%&gt;%addmargins</code></pre>
<pre><code>##      prediction
## truth No Medicaid Yes Medicaid  Sum
##   0            24         3980 4004
##   1            15          387  402
##   Sum          39         4367 4406</code></pre>
<pre class="r"><code>#class diag

class_diag(data$prob,data$y)</code></pre>
<pre><code>##         acc       sens     spec       ppv       auc
## 1 0.9067181 0.03731343 0.994006 0.3846154 0.8178789</code></pre>
<p><em>The class diag function returns the accuracy, sensitivity, specificity, and precision. The accuracy value of .91 indicates that 91% of the time we will get an accurate prediction. However, the low sensitivity of .04 indicates that this model is not good at predicting individuals who do have medicaid. This makes sense as medicaid is not just based off of income which is one of the variables that was fed into the model. This model is good at predicting if they don’t have medicaid with a specificity of .994, while the PPV is lower at .385. Overall, according to the AUC, this model is good at an auc of .818.</em></p>
<pre class="r"><code>#density plot

data$logit&lt;-predict(limlog)

ggplot(data,aes(logit, fill=y))+geom_density(alpha=.3)+geom_rug(aes(logit,color=y))+
  geom_vline(xintercept=0,lty=2)+theme(legend.position=c(.2,.8))+xlab(&quot;logit (log-odds)&quot;)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-13-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>data %&gt;% mutate(y=as.factor(medicaid)) %&gt;% ggplot() + geom_density(aes(logit, fill=y)) + geom_vline(xintercept=0)+xlab(&quot;predictor (logit)&quot;)</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-13-2.png" width="768" style="display: block; margin: auto;" />
<em>There are two density plots as I used one method we have used previously but no matter what I tweaked, I did not get a density plot that we saw previously. This graph shows that some of the points may be very rare and have </em></p>
<pre class="r"><code>#ROC
ROCplot&lt;-ggplot(data)+geom_roc(aes(d=y,m=prob), n.cuts=0)+geom_segment(aes(x=0,y=0,xend=1,yend=1),lty=2)+xlab(&quot;FPR&quot;)+ylab(&quot;TPR&quot;)
ROCplot</code></pre>
<p><img src="../../project/project2_files/figure-html/unnamed-chunk-14-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>calc_auc(ROCplot)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.8178911</code></pre>
<p><em>The AUC calculated here matched the AUC calculated by the Class Diag essentially confirming that this model is a good model of predicting medicaid from income and education with an AUC of .8179.</em></p>
</div>
<div id="logistic-model-all-variables" class="section level3">
<h3>Logistic model (All variables)</h3>
<pre class="r"><code>#model

data2 &lt;- Visits %&gt;% mutate(y = recode(medicaid, &quot;yes&quot; = 1, &quot;no&quot; = 0), insurance = recode(insurance, &quot;yes&quot; = 1, &quot;no&quot; = 0), afam= recode(afam, &quot;yes&quot; = 1, &quot;no&quot; = 0), adl = recode(adl, &quot;limited&quot; = 1, &quot;normal&quot; = 0), married = recode(married, &quot;yes&quot; = 1, &quot;no&quot; = 0), employed = recode(employed, &quot;yes&quot; = 1, &quot;no&quot; = 0)) %&gt;% mutate(school_c = school-mean(school), income_c= income- mean(income), age_c= age- mean(age), chronic_c= chronic- mean(chronic), cent_vis = total.healthcare.visits-mean(total.healthcare.visits))

log&lt;-glm(y~income_c+school_c+chronic_c+age_c+cent_vis+health+adl+region+afam+gender+married+employed+insurance, data=data2, family=binomial)
summary(log)</code></pre>
<pre><code>##
## Call:
## glm(formula = y ~ income_c + school_c + chronic_c +
age_c + cent_vis +
## health + adl + region + afam + gender + married +
employed +
## insurance, family = binomial, data = data2)
##
## Deviance Residuals:
## Min 1Q Median 3Q Max
## -2.1868 -0.2507 -0.1366 -0.0722 3.4901
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -1.979259 0.225839 -8.764 &lt; 2e-16 ***
## income_c -0.305227 0.054431 -5.608 2.05e-08 ***
## school_c -0.122515 0.018645 -6.571 5.00e-11 ***
## chronic_c 0.099762 0.049498 2.015 0.04386 *
## age_c -0.004180 0.010357 -0.404 0.68653
## cent_vis 0.008720 0.005263 1.657 0.09757 .
## healthexcellent -0.236965 0.330326 -0.717 0.47315
## healthpoor 0.234836 0.175489 1.338 0.18084
## adl 0.744408 0.154937 4.805 1.55e-06 ***
## regionnortheast 0.718356 0.239524 2.999 0.00271 **
## regionother 0.573905 0.208613 2.751 0.00594 **
## regionwest 1.472855 0.232481 6.335 2.37e-10 ***
## afam 0.482242 0.155838 3.095 0.00197 **
## gendermale -0.812448 0.166172 -4.889 1.01e-06 ***
## married -0.521942 0.163152 -3.199 0.00138 **
## employed -1.003902 0.415483 -2.416 0.01568 *
## insurance -2.740726 0.160913 -17.032 &lt; 2e-16 ***
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 2691.1 on 4405 degrees of freedom
## Residual deviance: 1533.2 on 4389 degrees of freedom
## AIC: 1567.2
##
## Number of Fisher Scoring iterations: 7</code></pre>
<pre class="r"><code>data2$prob &lt;- predict(log,type=&quot;response&quot;)

class_diag(data2$prob,data2$y)</code></pre>
<pre><code>##         acc      sens     spec       ppv       auc
## 1 0.9269178 0.4378109 0.976024 0.6470588 0.9274215</code></pre>
<p><em>The class diag function for the full set of variables returns the accuracy, sensitivity, specificity, and precision. The accuracy value of .927 indicates that 92.7% of the time we will get an accurate prediction of medicaid based off of all of the variables. This is slightly better than using the limited variables in the first logistical model. The sensitivity has also improved significantly from .04 to .438, which indicates this model is much better at predicting individuals who do have medicaid. However, this is still below .5. The specificity has reduced a bit from .994 to .976, showing that it is slightly worse at predicting who doesn’t have medicaid but it is still very good. The ppv has improved significantly from .385 to now .647. Overall, according to the AUC, this model is great at an auc of .927 which is better than the other model with an AUC of .8179. This is most likely because of the incorporation of all the other variables improving sensitivity.</em></p>
<pre class="r"><code>#10-fold

set.seed(348)
k=10
dat&lt;-data2[sample(nrow(data2)),] 
folds&lt;-cut(seq(1:nrow(data2)),breaks=k,labels=F)
diags&lt;-NULL
for(i in 1:k){
  train&lt;-dat[folds!=i,] 
  test&lt;-dat[folds==i,]
  
  truth&lt;-test$y
  
  fit3&lt;-glm(y~income_c+school_c+chronic_c+age_c+cent_vis+health+adl+region+afam+gender+married+employed+insurance, data=train, family=binomial)
  
  probs&lt;-predict(fit3,newdata = test,type=&quot;response&quot;)
  
  diags&lt;-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 1 0.9253303 0.4280769 0.9750076 0.6350023 0.9221913</code></pre>
<p><em>When performing 10-fold CV, overall the metrics of the model are very similar. Accuracy in 10 fold is .925 as compared to .927, sens is .428 vs .438, spec is .975 vs .976, and ppv is .635 vs .647. Overall, 10-fold cv seems to reduce the metric be a bit, which is reflected in its AUC being .9222 vs the non-10-fold AUC of .927.</em></p>
<pre class="r"><code>#lasso
data3 &lt;- data2
data3$health &lt;- factor(data3$health)
data3$region &lt;- factor(data3$region)

set.seed(348)
y&lt;-as.matrix(data3$y)
x&lt;-model.matrix(y~income_c+school_c+chronic_c+age_c+cent_vis+health+adl+region+afam+gender+married+employed+insurance,data=data3)[,-1]
x&lt;-scale(x)
cv&lt;-cv.glmnet(x,y, family= &quot;binomial&quot;)
lasso&lt;-glmnet(x,y,family=&quot;binomial&quot;,lambda=cv$lambda.1se)

coef(lasso)</code></pre>
<pre><code>## 17 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                           s0
## (Intercept)     -3.267653564
## income_c        -0.256152544
## school_c        -0.328741161
## chronic_c        0.048389295
## age_c            .          
## cent_vis         0.001045301
## healthexcellent  .          
## healthpoor       0.044749072
## adl              0.258773862
## regionnortheast  .          
## regionother      .          
## regionwest       0.134901921
## afam             0.108248805
## gendermale      -0.225608544
## married         -0.189824335
## employed         .          
## insurance       -1.040257082</code></pre>
<p><em>When doing a LASSO to retain the most important variables, we see age, employment status, excellent health status, and regions northeast and other being removed. I was surprised that age was removed however, could understand if its effects were monitorable in other variables such as health poor and chronic conditions. Similarly with employment status, I could see its effects on medicaid status being found in income. I was surprised that two but not all regions were removed. This could be that it is significant if you are in the west on medicaid status. Similarly with health excellent, it could mean that it does not matter if you are in excellent health as compared to average, but poor health matters.</em></p>
<pre class="r"><code>#lasso 10-fold

data3 &lt;-data3 %&gt;% mutate(healthpoor=ifelse(data3$health==&quot;poor&quot;,1,0), west=ifelse(data3$region==&quot;west&quot;,1,0))

set.seed(348)
k=10
dat&lt;-data3[sample(nrow(data3)),] 
folds&lt;-cut(seq(1:nrow(data3)),breaks=k,labels=F)
diags&lt;-NULL
for(i in 1:k){
  train&lt;-dat[folds!=i,] 
  test&lt;-dat[folds==i,]
  
  truth&lt;-test$y
  
  fit4&lt;-glm(y~income_c+school_c+chronic_c+cent_vis+healthpoor+adl+west+afam+gender+married+insurance, data=train, family=binomial)
  
  probs&lt;-predict(fit4,newdata = test,type=&quot;response&quot;)
  
  diags&lt;-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)</code></pre>
<pre><code>##         acc      sens      spec       ppv       auc
## 1 0.9244228 0.4084821 0.9760053 0.6345868 0.9219326</code></pre>
<p><em>When pulling out the variables and levels lasso said were not significant, the AUC slightly increased from 0.9221913 to 0.9219326. This was probably due to slight increases and decreases in other metrics. Overall, this probably means that the original model was still pretty good and not overfitting too much.</em></p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p><em>In conclusion, when analyzing various different interactions between different personal characteristics(I.E health, age, income, etc), we can create a model of the most pertinent characteristics to determine if someone is on medicaid or not. I would like to further this model by adding in other variables that determine medicaid eligibility, such as citizenship, number of children, SSI status, and more, to hopefully create a model that could more accurately predict medicaid status. Using this same data, it would be interesting to create a model for number of healthcare visits as that could also help individuals in budgeting for their upcoming year. Overall, this was an interesting project and I was glad to get significant results as last project I felt as if my results were not as interesting. Thank you again Professor for a great semester!</em></p>
</div>

            
        <hr>         <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div> 
            </div>
          </div>

   <hr>  <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div> 
        </div>
      </div>
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="../../js/docs.min.js"></script>
<script src="../../js/main.js"></script>

<script src="../../js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
